import boto3
import os
import sys
import time
from multiprocessing import Pool, cpu_count, Lock, Manager
from tqdm import tqdm

# Set your bucket names and AWS profiles
SOURCE_BUCKET = 'your-source-bucket-name'
DEST_BUCKET = 'your-destination-bucket-name'
LOG_FILE = 'copied_keys.log'
PREFIX = 'golden-online-metrics/'

# Set up boto3 clients globally so each process can access
s3_read = boto3.client('s3', profile_name='ReadOnly')
s3_write = boto3.client('s3', profile_name='PowerUser')

# Lock for writing logs from multiple processes
log_lock = Lock()

# Retry logic + copy
def copy_key(args):
    key, copied_keys = args

    if key in copied_keys:
        return key  # Already copied

    max_retries = 2
    for attempt in range(max_retries + 1):
        try:
            response = s3_read.get_object(Bucket=SOURCE_BUCKET, Key=key)
            body = response['Body']
            s3_write.upload_fileobj(body, DEST_BUCKET, key)

            with log_lock:
                with open(LOG_FILE, 'a') as log:
                    log.write(f"{key}\n")
            return key  # success
        except Exception as e:
            if attempt < max_retries:
                time.sleep(2 ** attempt)  # exponential backoff
            else:
                print(f"❌ Failed after retries: {key} - {e}", file=sys.stderr)
                return None

def main():
    # Load previously copied keys
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, 'r') as f:
            copied_keys = set(line.strip() for line in f)
    else:
        copied_keys = set()

    print("🔍 Fetching list of objects...")
    paginator = s3_read.get_paginator('list_objects_v2')
    all_keys = []
    for page in paginator.paginate(Bucket=SOURCE_BUCKET, Prefix=PREFIX):
        all_keys += [obj['Key'] for obj in page.get('Contents', [])]

    keys_to_copy = [key for key in all_keys if key not in copied_keys]
    print(f"📦 Total files to copy: {len(keys_to_copy)}")

    # Multiprocessing pool with tqdm
    with Manager() as manager:
        shared_copied = manager.dict()
        args = [(key, copied_keys) for key in keys_to_copy]

        with Pool(processes=cpu_count()) as pool:
            for _ in tqdm(pool.imap_unordered(copy_key, args), total=len(args), desc="Copying files", unit="file"):
                pass

    print("✅ All uncopied files streamed and saved successfully.")

if __name__ == "__main__":
    main()