from pyspark.sql import SparkSession
from pyspark.sql.functions import year, month, dayofmonth, hour, current_timestamp
import argparse

def compare_schemas(df_schema, table_schema):
    df_fields = {f.name: f.dataType.simpleString() for f in df_schema}
    table_fields = {f.name: f.dataType.simpleString() for f in table_schema}

    print("üîç Comparing schemas...\n")

    only_in_df = set(df_fields) - set(table_fields)
    only_in_table = set(table_fields) - set(df_fields)
    mismatched_types = {
        col: (df_fields[col], table_fields[col])
        for col in set(df_fields) & set(table_fields)
        if df_fields[col] != table_fields[col]
    }

    if only_in_df:
        print(f"‚ö†Ô∏è Columns only in DataFrame: {sorted(only_in_df)}")
    if only_in_table:
        print(f"‚ö†Ô∏è Columns only in Hive Table: {sorted(only_in_table)}")
    if mismatched_types:
        print(f"‚ö†Ô∏è Type mismatches:")
        for col, (df_type, table_type) in mismatched_types.items():
            print(f"   - Column: {col} | DF Type: {df_type} | Table Type: {table_type}")

    return not (only_in_df or only_in_table or mismatched_types)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", required=True)
    parser.add_argument("--output_path", required=True)
    parser.add_argument("--table_name", required=True)
    args = parser.parse_args()

    spark = SparkSession.builder \
        .appName("SchemaValidatorAndWriter") \
        .enableHiveSupport() \
        .getOrCreate()

    # Step 1: Load data
    df = spark.read.parquet(args.input_path) \
        .withColumn("run_year", year(current_timestamp())) \
        .withColumn("run_month", month(current_timestamp())) \
        .withColumn("run_day", dayofmonth(current_timestamp())) \
        .withColumn("run_hour", hour(current_timestamp()))

    print("üì¶ Inferred DataFrame Schema:")
    df.printSchema()

    # Step 2: Try comparing with existing table
    try:
        table_df = spark.table(args.table_name)
        print(f"\nüìö Existing Hive Table Schema for {args.table_name}:")
        table_df.printSchema()
        compatible = compare_schemas(df.schema, table_df.schema)
    except Exception as e:
        print(f"‚ö†Ô∏è Could not read Hive table {args.table_name}. Assuming new table. Error: {e}")
        compatible = True

    # Step 3: Write only if schema is compatible
    if compatible:
        print("üöÄ Writing DataFrame to table...")
        df.write \
            .format("parquet") \
            .mode("overwrite") \
            .partitionBy("run_year", "run_month", "run_day", "run_hour") \
            .option("path", args.output_path) \
            .saveAsTable(args.table_name)
        print(f"‚úÖ Successfully saved table: {args.table_name}")
    else:
        print("‚ùå Schema mismatch ‚Äî skipping write.")

if __name__ == "__main__":
    main()