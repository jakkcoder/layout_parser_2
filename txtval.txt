def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_path", required=True)
    parser.add_argument("--output_path", required=True)
    parser.add_argument("--table_name", required=True)
    args = parser.parse_args()

    spark = SparkSession.builder \
        .appName("Register abandonment_metrics_table") \
        .enableHiveSupport() \
        .getOrCreate()

    # Dynamically get schema from Parquet files
    inferred_schema = spark.read.parquet(args.input_path).schema

    # Read the actual data using inferred schema
    df = spark.read.schema(inferred_schema).parquet(args.input_path) \
        .withColumn("run_year", year(current_timestamp())) \
        .withColumn("run_month", month(current_timestamp())) \
        .withColumn("run_day", dayofmonth(current_timestamp())) \
        .withColumn("run_hour", hour(current_timestamp()))

    # Write the data with partitioning and overwrite mode
    df.write \
        .format("parquet") \
        .mode("overwrite") \
        .partitionBy("run_year", "run_month", "run_day", "run_hour") \
        .option("path", args.output_path) \
        .saveAsTable(args.table_name)

    print(f"âœ… Registered table: {args.table_name} at {args.output_path}")